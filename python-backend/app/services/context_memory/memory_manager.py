"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é
"""
import asyncio
from typing import List, Dict, Optional, Any, Callable
from datetime import datetime, timedelta
import uuid

from .models import (
    MemoryItem, MemoryType, ImportanceLevel, 
    MemoryStats, ContextWindow, Summary
)
from .vector_store import VectorMemoryStore
from .summarizer import ContextSummarizer
from .compression import ContextCompressor

class MemoryManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–º—è—Ç–∏ - —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–π —Å–∏—Å—Ç–µ–º–æ–π –ø–∞–º—è—Ç–∏
    """
    
    def __init__(self,
                 vector_store: Optional[VectorMemoryStore] = None,
                 summarizer: Optional[ContextSummarizer] = None,
                 conversation_id: str = "default"):
        
        self.vector_store = vector_store
        self.summarizer = summarizer
        self.conversation_id = conversation_id
        
        # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (—Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        self.short_term: List[MemoryItem] = []
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        self.context_window = ContextWindow(max_tokens=4000)  # 4K —Ç–æ–∫–µ–Ω–æ–≤
        
        # –ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä
        self.compressor = ContextCompressor(summarizer)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "short_term_items": 0,
            "long_term_items": 0,
            "summaries_created": 0,
            "context_compressions": 0
        }
        
        # –ö–æ–ª–±—ç–∫–∏
        self.callbacks: List[Callable] = []
        
        # –ó–∞–¥–∞—á–∏
        self._maintenance_task = None
        self._running = False
    
    def on_memory_update(self, callback: Callable[[MemoryItem], None]):
        """–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        self.callbacks.append(callback)
    
    async def start(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä"""
        self._running = True
        self._maintenance_task = asyncio.create_task(self._maintenance_loop())
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä"""
        self._running = False
        if self._maintenance_task:
            self._maintenance_task.cancel()
            try:
                await self._maintenance_task
            except asyncio.CancelledError:
                pass
    
    async def add_message(self,
                         content: str,
                         sender: str,
                         importance: ImportanceLevel = ImportanceLevel.MEDIUM,
                         memory_type: MemoryType = MemoryType.SHORT_TERM,
                         metadata: Optional[Dict] = None) -> MemoryItem:
        """
        –î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
        """
        # –°–æ–∑–¥–∞—ë–º —ç–ª–µ–º–µ–Ω—Ç –ø–∞–º—è—Ç–∏
        memory = MemoryItem(
            id=f"mem_{datetime.now().timestamp()}_{uuid.uuid4().hex[:8]}",
            content=content,
            type=memory_type,
            importance=importance,
            timestamp=datetime.now(),
            metadata=metadata or {},
            tags=self._extract_tags(content),
            participants=[sender],
            ttl=3600 if memory_type == MemoryType.SHORT_TERM else None  # 1 —á–∞—Å –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å
        self.short_term.append(memory)
        self.stats["short_term_items"] = len(self.short_term)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        tokens = len(content.split()) * 1.3
        should_compress = self.context_window.add_message({
            "id": memory.id,
            "sender": sender,
            "content": content,
            "timestamp": memory.timestamp,
            "conversation_id": self.conversation_id
        }, int(tokens))
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if should_compress:
            await self.compress_context()
        
        # –ï—Å–ª–∏ –≤–∞–∂–Ω–æ–µ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é
        if importance in [ImportanceLevel.HIGH, ImportanceLevel.CRITICAL]:
            await self.transfer_to_long_term(memory)
        
        # –í—ã–∑—ã–≤–∞–µ–º –∫–æ–ª–±—ç–∫–∏
        for callback in self.callbacks:
            try:
                callback(memory)
            except Exception as e:
                print(f"Error in callback: {e}")
        
        return memory
    
    async def transfer_to_long_term(self, memory: MemoryItem):
        """
        –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å
        """
        if not self.vector_store:
            return
        
        # –ú–µ–Ω—è–µ–º —Ç–∏–ø
        memory.type = MemoryType.LONG_TERM
        memory.ttl = None  # –±–µ—Å—Å—Ä–æ—á–Ω–æ
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store.add_memory(memory)
        self.stats["long_term_items"] = self.vector_store.collection.count()
    
    async def compress_context(self):
        """
        –°–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        # –°–∂–∏–º–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        self.compressor.compress_context(self.context_window)
        self.stats["context_compressions"] += 1
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏
        self.short_term = [m for m in self.short_term if not m.is_expired()]
        self.stats["short_term_items"] = len(self.short_term)
    
    async def search_memory(self, 
                           query: str,
                           include_short_term: bool = True,
                           include_long_term: bool = True,
                           n_results: int = 5) -> List[MemoryItem]:
        """
        –ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏
        """
        results = []
        
        # –ü–æ–∏—Å–∫ –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π
        if include_short_term:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_words = set(query.lower().split())
            for mem in self.short_term:
                mem_words = set(mem.content.lower().split())
                overlap = len(query_words & mem_words)
                if overlap > 0:
                    results.append((overlap, mem))
            
            results.sort(key=lambda x: x[0], reverse=True)
        
        # –ü–æ–∏—Å–∫ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π
        if include_long_term and self.vector_store:
            vector_results = self.vector_store.search_memory(
                query=query,
                n_results=n_results
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MemoryItem
            for vr in vector_results:
                mem = MemoryItem(
                    id=vr["id"],
                    content=vr["content"],
                    type=MemoryType.LONG_TERM,
                    importance=ImportanceLevel(vr["metadata"]["importance"]),
                    timestamp=datetime.fromisoformat(vr["metadata"]["timestamp"]),
                    tags=vr["metadata"].get("tags", [])
                )
                results.append((vr.get("distance", 0), mem))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        seen = set()
        unique_results = []
        for score, mem in results:
            if mem.id not in seen:
                seen.add(mem.id)
                unique_results.append(mem)
        
        return unique_results[:n_results]
    
    def get_relevant_context(self, query: str, max_tokens: int = 1000) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        """
        # –ò—â–µ–º –≤ –ø–∞–º—è—Ç–∏
        memories = asyncio.run(self.search_memory(
            query=query,
            n_results=5
        ))
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return self.compressor.get_optimal_context(
            query=query,
            recent_messages=self.context_window.messages[-20:],  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20
            vector_memories=memories,
            max_tokens=max_tokens
        )
    
    async def create_summary(self, chunk_size: int = 50) -> Optional[Summary]:
        """
        –°–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        if not self.summarizer or len(self.context_window.messages) < chunk_size:
            return None
        
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = self.context_window.messages[-chunk_size:]
        
        # –°–æ–∑–¥–∞—ë–º —á–∞–Ω–∫
        from .models import ConversationChunk
        
        chunk = ConversationChunk(
            chunk_id=f"auto_summary_{datetime.now().timestamp()}",
            conversation_id=self.conversation_id,
            messages=messages,
            start_time=messages[0]['timestamp'],
            end_time=messages[-1]['timestamp'],
            participants=list(set(m['sender'] for m in messages)),
            token_count=int(sum(len(m['content'].split()) * 1.3 for m in messages))
        )
        
        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º
        summary = await self.summarizer.summarize_chunk(chunk)
        
        if summary:
            self.stats["summaries_created"] += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å
            memory = MemoryItem(
                id=f"summary_{summary.summary_id}",
                content=summary.content,
                type=MemoryType.SEMANTIC,
                importance=ImportanceLevel.HIGH,
                timestamp=datetime.now(),
                tags=["summary", "auto_generated"],
                participants=chunk.participants,
                metadata={
                    "summary_id": summary.summary_id,
                    "key_points": summary.key_points,
                    "decisions": summary.decisions
                }
            )
            
            await self.transfer_to_long_term(memory)
        
        return summary
    
    async def _maintenance_loop(self):
        """–§–æ–Ω–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        while self._running:
            await asyncio.sleep(300)  # –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
            self.short_term = [m for m in self.short_term if not m.is_expired()]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ—Ä–∞ –ª–∏ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–∫—É
            if len(self.context_window.messages) > 100:
                await self.create_summary(chunk_size=50)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats["short_term_items"] = len(self.short_term)
    
    def _extract_tags(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–≥–∏ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        tags = []
        
        # –•—ç—à—Ç–µ–≥–∏
        import re
        hashtags = re.findall(r'#(\w+)', content)
        tags.extend(hashtags)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        important_words = ['–≤–∞–∂–Ω–æ', '—Å—Ä–æ—á–Ω–æ', '—Ä–µ—à–µ–Ω–∏–µ', '–∏—Ç–æ–≥']
        for word in important_words:
            if word in content.lower():
                tags.append(word)
        
        return tags
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            **self.stats,
            "context_window": {
                "messages": len(self.context_window.messages),
                "tokens": self.context_window.current_tokens,
                "summaries": len(self.context_window.summaries)
            },
            "vector_store_stats": self.vector_store.get_stats() if self.vector_store else None,
            "summarizer_stats": self.summarizer.get_stats() if self.summarizer else None
        }
    
    def get_memory_summary(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –ø–∞–º—è—Ç–∏"""
        lines = [
            f"üìä **–°–≤–æ–¥–∫–∞ –ø–∞–º—è—Ç–∏**",
            f"–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å: {len(self.short_term)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤",
            f"–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å: {self.stats['long_term_items']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤",
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ: {len(self.context_window.messages)} —Å–æ–æ–±—â–µ–Ω–∏–π / {self.context_window.current_tokens} —Ç–æ–∫–µ–Ω–æ–≤",
            f"–°–æ–∑–¥–∞–Ω–æ —Å–≤–æ–¥–æ–∫: {self.stats['summaries_created']}",
            f"–ö–æ–º–ø—Ä–µ—Å—Å–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {self.stats['context_compressions']}"
        ]
        
        if self.context_window.summaries:
            lines.append("\nüìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–≤–æ–¥–∫–∏:")
            for s in self.context_window.summaries[-3:]:
                lines.append(f"  ‚Ä¢ {s.key_points[0] if s.key_points else s.content[:50]}...")
        
        return "\n".join(lines)
